import streamlit as st
import time
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import load_prompt
from dotenv import load_dotenv
import os

# API KEY ì •ë³´ ë¡œë“œ
load_dotenv()

# Streamlit ì œëª©
st.title("Mindful Companion")

# ğŸš€ ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
if st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”"):
    st.session_state["memory"] = ConversationBufferMemory()  # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨

# ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì„¸ì…˜ ìƒíƒœ í™œìš©)
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory()

# OpenAI GPT-4o ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
if "chat_model" not in st.session_state:
    st.session_state["chat_model"] = ChatOpenAI(model_name="gpt-4o", temperature=0.6, streaming=True)

# í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ
prompt_path = "mindful_companion.yaml"
loaded_prompt = load_prompt(prompt_path, encoding="utf8")

# LangChain ì²´ì¸ ì„¤ì •
llm = st.session_state["chat_model"]
chain = LLMChain(llm=llm, prompt=loaded_prompt)

# ì´ì „ ëŒ€í™” ì¶œë ¥
for message in st.session_state["memory"].chat_memory.messages:
    role = "user" if isinstance(message, HumanMessage) else "assistant"
    with st.chat_message(role):
        st.markdown(message.content)

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

# ğŸš€ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input:
    # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì¦‰ì‹œ í™”ë©´ì— ì¶œë ¥
    st.chat_message("user").write(user_input)

    # ì‚¬ìš©ì ì…ë ¥ì„ ë©”ëª¨ë¦¬ì— ì¶”ê°€
    st.session_state["memory"].chat_memory.add_user_message(user_input)

    # ëŒ€í™” ë‚´ì—­ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
    chat_history_str = "\n".join(
        [msg.content for msg in st.session_state["memory"].chat_memory.messages]
    )

    # ğŸš€ AI ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥ (í•œ í† í°ì”©)
    with st.chat_message("assistant"):
        container = st.empty()  # ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±
        response = ""
        
        chunk_size = 10  # í•œ ë²ˆì— ì¶œë ¥í•  ë¸”ë¡ í¬ê¸°
        buffer = ""
        
        for chunk in chain.stream({"chat_history": chat_history_str, "question": user_input, "context": chat_history_str}):
            text_chunk = chunk["text"] if isinstance(chunk, dict) and "text" in chunk else str(chunk)
            buffer += text_chunk
            
            # ì¼ì • í¬ê¸°ì˜ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ìŒ“ì•„ë‘” í›„ ì¶œë ¥
            if len(buffer) >= chunk_size:
                response += buffer
                container.write(response)  # ì ì§„ì  ì—…ë°ì´íŠ¸
                buffer = ""  # ë²„í¼ ì´ˆê¸°í™”
                
        # ë‚¨ì•„ ìˆëŠ” ë²„í¼ ì¶œë ¥
        if buffer:
            response += buffer
            container.write(response)
        
        # ìµœì¢… ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì¶”ê°€
        st.session_state["memory"].chat_memory.add_ai_message(response)
